{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c2613a9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e56884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "\n",
    "# \n",
    "import unicodedata\n",
    "import json\n",
    "from time import strftime\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Custom\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d65799",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "**Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:**\n",
    "\n",
    "   - Lowercase everything\n",
    "   - Normalize unicode characters\n",
    "   - Replace anything that is not a letter, number, whitespace or a single quote.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba85a182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one evening as the sun went down\\nand the jungle fire was burning\\ndown the track came a hobo hiking\\nand he said, \"boys, i\\'m not turning\"\\n\"i\\'m headed for a land that\\'s far away\\nbesides the crystal fountains\\nso come with me, we\\'ll go and see\\nthe big rock candy mountains\"\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample String\n",
    "og_string = '''One evening as the sun went down\n",
    "And the jungle fire was burning\n",
    "Down the track came a hobo hiking\n",
    "And he said, \"Boys, I'm not turning\"\n",
    "\"I'm headed for a land that's far away\n",
    "Besides the crystal fountains\n",
    "So come with me, we'll go and see\n",
    "The Big Rock Candy Mountains\"\n",
    "'''\n",
    "og_string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fb2f0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One evening as the sun went down\\nAnd the jungle fire was burning\\nDown the track came a hobo hiking\\nAnd he said, \"Boys, I\\'m not turning\"\\n\"I\\'m headed for a land that\\'s far away\\nBesides the crystal fountains\\nSo come with me, we\\'ll go and see\\nThe Big Rock Candy Mountains\"\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unicodedata.normalize\n",
    "string = unicodedata.normalize('NFKD', og_string)\\\n",
    "             .encode('ascii', 'ignore')\\\n",
    "             .decode('utf-8', 'ignore')\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a0c867d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one evening as the sun went down\\nand the jungle fire was burning\\ndown the track came a hobo hiking\\nand he said boys im not turning\\nim headed for a land thats far away\\nbesides the crystal fountains\\nso come with me well go and see\\nthe big rock candy mountains\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace anything that is not a letter, number, whitespace or a single quote\n",
    "re.sub(r'[^\\w\\s]', '', string).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3f8665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the function\n",
    "def basic_clean(string):\n",
    "    '''\n",
    "    Takes in any string and\n",
    "    returns the string normalized.\n",
    "    '''\n",
    "    string = unicodedata.normalize('NFKD', og_string)\\\n",
    "             .encode('ascii', 'ignore')\\\n",
    "             .decode('utf-8', 'ignore')\n",
    "    string = re.sub(r'[^\\w\\s]', '', string).lower()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696fc848",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "**Define a function named tokenize. It should take in a string and tokenize all the words in the string.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35dda644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One evening as the sun went down\n",
      "And the jungle fire was burning\n",
      "Down the track came a hobo hiking\n",
      "And he said , \" Boys , I ' m not turning \" \n",
      " \" I ' m headed for a land that ' s far away\n",
      "Besides the crystal fountains\n",
      "So come with me , we ' ll go and see\n",
      "The Big Rock Candy Mountains \"\n"
     ]
    }
   ],
   "source": [
    "# Make the tokenizer\n",
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "# Use it\n",
    "print(tokenizer.tokenize(og_string, return_str=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d374025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the function\n",
    "def tokenize(string):\n",
    "    '''\n",
    "    Takes in a string and\n",
    "    returns a tokenized string\n",
    "    '''\n",
    "    # Create tokenizer.\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    # Use tokenizer\n",
    "    string = tokenizer.tokenize(string, return_str = True)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593721ac",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "**Define a function named stem. It should accept some text and return the text after applying stemming to all the words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede33184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c116aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns a string with words stemmed.\n",
    "    '''\n",
    "    # Create porter stemmer.\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    # Use the stemmer to stem each word in the list of words we created by using split.\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    # Join our lists of words into a string again and assign to a variable.\n",
    "    string = ' '.join(stems)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3edc99",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "**Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f45334f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e05669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf46370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    '''\n",
    "    Takes in string and\n",
    "    returns a string with words lemmatized.\n",
    "    '''\n",
    "    # Create the lemmatizer.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # Use the lemmatizer on each word in the list of words we created by using split.\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    \n",
    "    # Join our list of words into a string again and assign to a variable.\n",
    "    string = ' '.join(lemmas)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25202f94",
   "metadata": {},
   "source": [
    "# Exercise 5\n",
    "**Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.**\n",
    "\n",
    "**This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a1338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caab893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cfae3eb",
   "metadata": {},
   "source": [
    "# Exercise 6\n",
    "**Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff46e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570eabda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "871cb722",
   "metadata": {},
   "source": [
    "# Exercise 7\n",
    "**Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca474c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c74cea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d89c1df4",
   "metadata": {},
   "source": [
    "# Exercise 8\n",
    "**For each dataframe, produce the following columns:**\n",
    "\n",
    "   - title to hold the title\n",
    "   - original to hold the original article/post content\n",
    "   - clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "   - stemmed to hold the stemmed version of the cleaned data.\n",
    "   - lemmatized to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa1f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3920a5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cde480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0032be65",
   "metadata": {},
   "source": [
    "# Exercise 9\n",
    "\n",
    "**Ask yourself:**\n",
    "    \n",
    "   - If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "   - If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "   - If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0e99c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
